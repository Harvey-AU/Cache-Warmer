{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Website Cache Warmer Settings\n",
    "\n",
    "#@markdown ### Enter the website URL you want to warm the cache for:\n",
    "url = \"https://riaa.webflow.io/\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Configure crawling behavior:\n",
    "concurrency = 11 #@param {type:\"slider\", min:1, max:50, step:5}\n",
    "delay = 1.4 #@param {type:\"slider\", min:0.1, max:3.0, step:0.1}\n",
    "max_pages = 2501 #@param {type:\"slider\", min:1, max:2501, step:50}\n",
    "update_sheet_interval = 45 #@param {type:\"slider\", min:5, max:505, step:10}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ### Additional Options:\n",
    "use_sitemap = True #@param {type:\"boolean\"}\n",
    "verbose = False #@param {type:\"boolean\"}\n",
    "find_links = False #@param {type:\"boolean\"}\n",
    "continue_from_last = False #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown Exclude Paths: Skip URLs containing these paths (comma-separated, e.g. \"admin,draft,test\")\n",
    "exclude_paths = \"\" #@param {type:\"string\"}\n",
    "#@markdown Include Paths: Only process URLs containing these paths (comma-separated, e.g. \"blog,products\")\n",
    "include_paths = \"\" #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import concurrent.futures\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from google.colab import auth\n",
    "import gspread\n",
    "from google.auth import default\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "\n",
    "def log_system_metrics():\n",
    "    cpu = psutil.cpu_percent(interval=1)\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] CPU: {cpu}%, Memory: {mem.percent}% used\")\n",
    "\n",
    "\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    print(\"For better visualization, install tqdm: pip install tqdm\")\n",
    "    # Fallback if tqdm not available\n",
    "    class tqdm:\n",
    "        def __init__(self, total, desc):\n",
    "            self.total = total\n",
    "            self.desc = desc\n",
    "            self.n = 0\n",
    "            print(f\"{desc}: 0/{total}\")\n",
    "\n",
    "        def update(self, n):\n",
    "            self.n += na\n",
    "            print(f\"\\r{self.desc}: {self.n}/{self.total}\", end=\"\")\n",
    "\n",
    "        def close(self):\n",
    "            print()\n",
    "\n",
    "class SiteCacheWarmer:\n",
    "    def __init__(self, start_url, concurrency=5, delay=0.5, max_pages=100, verbose=True,\n",
    "                 use_sitemap=True, max_retries=3, exclude_paths=None, include_paths=None, find_links=True,\n",
    "                 update_interval=50, continue_from_last=False):\n",
    "        self.start_url = start_url\n",
    "        self.base_domain = urlparse(start_url).netloc\n",
    "        self.visited_urls = set()\n",
    "        self.queue = [start_url]\n",
    "        self.concurrency = concurrency\n",
    "        self.delay = delay\n",
    "        self.max_pages = max_pages\n",
    "        self.verbose = verbose\n",
    "        self.use_sitemap = use_sitemap\n",
    "        self.max_retries = max_retries\n",
    "        self.exclude_paths = exclude_paths\n",
    "        self.include_paths = include_paths\n",
    "        self.find_links = find_links\n",
    "        self.retry_counts = {}\n",
    "        self.page_times = {}\n",
    "        self.status_codes = {}\n",
    "        self.progress_bar = None\n",
    "        self.url_from_sitemap = {}\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "        self.update_interval = update_interval\n",
    "        self.last_updated_count = 0\n",
    "        self.gs_headers_written = False\n",
    "        self.worksheet = None\n",
    "        self.continue_from_last = continue_from_last\n",
    "\n",
    "    def _init_google_sheet(self):\n",
    "      auth.authenticate_user()\n",
    "      creds, _ = default()\n",
    "      gc = gspread.authorize(creds)\n",
    "      from googleapiclient.discovery import build\n",
    "      drive_service = build('drive', 'v3', credentials=creds)\n",
    "      self.drive_service = drive_service\n",
    "      central_folder_id = \"1Om8LfyP7WVRGT07KK4ggn5CA7-HfjlaF\"\n",
    "      query = f\"'{central_folder_id}' in parents and mimeType = 'application/vnd.google-apps.folder' and name = '{self.base_domain}'\"\n",
    "      response = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "      folders = response.get('files', [])\n",
    "      if folders:\n",
    "          domain_folder_id = folders[0]['id']\n",
    "      else:\n",
    "          folder_metadata = {\n",
    "              'name': self.base_domain,\n",
    "              'mimeType': 'application/vnd.google-apps.folder',\n",
    "              'parents': [central_folder_id]\n",
    "          }\n",
    "          folder = drive_service.files().create(body=folder_metadata, fields='id').execute()\n",
    "          domain_folder_id = folder.get('id')\n",
    "          print(f\"Created domain subfolder: {self.base_domain} (ID: {domain_folder_id})\")\n",
    "      timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "      sheet_name = f\"{self.base_domain} - {timestamp}\"\n",
    "      file_metadata = {\n",
    "          'name': sheet_name,\n",
    "          'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "          'parents': [domain_folder_id]\n",
    "      }\n",
    "      file = drive_service.files().create(\n",
    "          body=file_metadata,\n",
    "          fields='id, webViewLink',\n",
    "          supportsAllDrives=True\n",
    "      ).execute()\n",
    "      print(\"Google Sheet created:\", file.get('webViewLink'))\n",
    "      spreadsheet = gc.open_by_key(file.get('id'))\n",
    "      self.worksheet = spreadsheet.get_worksheet(0)\n",
    "\n",
    "    def _load_last_google_sheet(self):\n",
    "      auth.authenticate_user()\n",
    "      creds, _ = default()\n",
    "      from googleapiclient.discovery import build\n",
    "      drive_service = build('drive', 'v3', credentials=creds)\n",
    "      central_folder_id = \"1Om8LfyP7WVRGT07KK4ggn5CA7-HfjlaF\"\n",
    "      query_folder = f\"'{central_folder_id}' in parents and mimeType = 'application/vnd.google-apps.folder' and name = '{self.base_domain}'\"\n",
    "      response_folder = drive_service.files().list(q=query_folder, fields=\"files(id, name)\").execute()\n",
    "      folders = response_folder.get('files', [])\n",
    "      if not folders:\n",
    "          print(\"No domain subfolder found. Starting fresh.\")\n",
    "          return\n",
    "      domain_folder_id = folders[0]['id']\n",
    "      query = f\"'{domain_folder_id}' in parents and mimeType='application/vnd.google-apps.spreadsheet'\"\n",
    "      response = drive_service.files().list(q=query, orderBy=\"createdTime desc\", pageSize=1, fields=\"files(id, name)\").execute()\n",
    "      files = response.get('files', [])\n",
    "      if not files:\n",
    "          print(\"No previous Google Sheet found for this domain. Starting fresh.\")\n",
    "          return\n",
    "      latest_file = files[0]\n",
    "      gc = gspread.authorize(creds)\n",
    "      spreadsheet = gc.open_by_key(latest_file['id'])\n",
    "      worksheet = spreadsheet.get_worksheet(0)\n",
    "      data = worksheet.get_all_values()\n",
    "      if not data or len(data) < 2:\n",
    "          print(\"Google Sheet is empty. Starting fresh.\")\n",
    "          return\n",
    "      header = data[0]\n",
    "      url_index = header.index(\"URL\")\n",
    "      loaded_urls = set(row[url_index] for row in data[1:] if row[url_index])\n",
    "      self.visited_urls = loaded_urls\n",
    "      self.last_updated_count = len(data) - 1\n",
    "      print(f\"Resuming from previous run: Loaded {len(loaded_urls)} visited URLs from sheet '{latest_file['name']}'.\")\n",
    "\n",
    "    def _update_google_sheet_incremental(self):\n",
    "      data = {\n",
    "          'URL': list(self.visited_urls),\n",
    "          'Load Time (s)': [self.page_times.get(url) for url in self.visited_urls],\n",
    "          'Status': [str(self.status_codes.get(url)) for url in self.visited_urls],\n",
    "          'Retries': [self.retry_counts.get(url, 0) for url in self.visited_urls],\n",
    "          'URL Source': [(\"Sitemap\" if self.url_from_sitemap.get(url, False) else \"Discovered\") for url in self.visited_urls]\n",
    "      }\n",
    "      df = pd.DataFrame(data)\n",
    "      df = df.sort_values('URL')\n",
    "      new_df = df.iloc[self.last_updated_count:]\n",
    "      new_df = new_df.where(pd.notnull(new_df), None)\n",
    "      if not new_df.empty:\n",
    "          rows = new_df.values.tolist()\n",
    "          if not self.gs_headers_written:\n",
    "              header = new_df.columns.tolist()\n",
    "              self.worksheet.append_row(header)\n",
    "              self.gs_headers_written = True\n",
    "          self.worksheet.append_rows(rows, value_input_option='USER_ENTERED')\n",
    "          self.last_updated_count = len(df)\n",
    "          print(f\"Incremental update: Appended {len(rows)} rows. Total processed: {self.last_updated_count}\")\n",
    "\n",
    "\n",
    "    def print_if_verbose(self, message):\n",
    "        if self.verbose:\n",
    "            print(message)\n",
    "\n",
    "    def parse_sitemap(self, base_url, exclude_paths=None, include_paths=None):\n",
    "        try:\n",
    "            sitemap_url = urljoin(base_url, 'sitemap.xml')\n",
    "            print(f\"Attempting to parse sitemap from: {sitemap_url}\")\n",
    "            response = requests.get(sitemap_url, headers=self.headers, timeout=30)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to get sitemap (status {response.status_code}). Falling back to normal crawling.\")\n",
    "                return []\n",
    "            content_type = response.headers.get('Content-Type', '')\n",
    "            if 'gzip' in content_type or 'application/x-gzip' in content_type:\n",
    "                content = gzip.decompress(response.content)\n",
    "            else:\n",
    "                content = response.content\n",
    "            root = ET.parse(BytesIO(content)).getroot()\n",
    "            urls = []\n",
    "            namespaces = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            url_elements = root.findall('.//sm:url', namespaces)\n",
    "            if not url_elements:\n",
    "                url_elements = root.findall('.//url')\n",
    "            for url_elem in url_elements:\n",
    "                loc = url_elem.find('.//sm:loc', namespaces)\n",
    "                if loc is None:\n",
    "                    loc = url_elem.find('.//loc')\n",
    "                if loc is not None and loc.text:\n",
    "                    full_url = loc.text.strip()\n",
    "                    parsed_url = urlparse(full_url)\n",
    "                    path = parsed_url.path\n",
    "                    if exclude_paths and any(excluded in path for excluded in exclude_paths):\n",
    "                        continue\n",
    "                    if include_paths:\n",
    "                        if not any(included in path for included in include_paths):\n",
    "                            continue\n",
    "                    urls.append(full_url)\n",
    "            print(f\"Successfully found {len(urls)} URLs in sitemap after filtering.\")\n",
    "            return urls\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing sitemap: {str(e)}. Falling back to normal crawling.\")\n",
    "            return []\n",
    "\n",
    "    def is_valid_url(self, url):\n",
    "        if not url.startswith('http'):\n",
    "            return False\n",
    "        parsed_url = urlparse(url)\n",
    "        if parsed_url.netloc != self.base_domain:\n",
    "            return False\n",
    "        if not parsed_url.path or parsed_url.path == '/':\n",
    "            if parsed_url.query == '' and self.start_url in self.visited_urls:\n",
    "                return False\n",
    "        extensions_to_skip = ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.zip', '.css', '.js']\n",
    "        if any(url.lower().endswith(ext) for ext in extensions_to_skip):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def extract_links(self, html, base_url):\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            full_url = urljoin(base_url, href)\n",
    "            parsed = urlparse(full_url)\n",
    "            clean_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "            if parsed.query:\n",
    "                clean_url += f\"?{parsed.query}\"\n",
    "            if self.is_valid_url(clean_url):\n",
    "                links.append(clean_url)\n",
    "        return links\n",
    "\n",
    "    def visit_url(self, url):\n",
    "        if url not in self.retry_counts:\n",
    "            self.retry_counts[url] = 0\n",
    "        current_try = self.retry_counts[url] + 1\n",
    "        try:\n",
    "            if current_try > 1 and self.verbose:\n",
    "                print(f\"Retry #{current_try-1} for {url}\")\n",
    "            start_time = time.time()\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            duration = time.time() - start_time\n",
    "            self.page_times[url] = duration\n",
    "            self.status_codes[url] = response.status_code\n",
    "            if self.verbose:\n",
    "                print(f\"✓ {url} - Status: {response.status_code}, Time: {duration:.2f}s\")\n",
    "            if 'text/html' in response.headers.get('Content-Type', ''):\n",
    "                if response.status_code == 200:\n",
    "                    links = self.extract_links(response.text, url) if self.find_links else []\n",
    "                    return url, links\n",
    "            return url, []\n",
    "        except requests.exceptions.Timeout:\n",
    "            self.retry_counts[url] += 1\n",
    "            if self.retry_counts[url] < self.max_retries:\n",
    "                if self.verbose:\n",
    "                    print(f\"⚠ Timeout for {url}. Will retry ({self.retry_counts[url]}/{self.max_retries})...\")\n",
    "                return url, [url]\n",
    "            self.page_times[url] = None\n",
    "            self.status_codes[url] = 'Timeout'\n",
    "            if self.verbose:\n",
    "                print(f\"✗ Timeout accessing {url} after {self.max_retries} tries.\")\n",
    "            return url, []\n",
    "        except Exception as e:\n",
    "            self.page_times[url] = None\n",
    "            self.status_codes[url] = f'Error: {type(e).__name__}'\n",
    "            if self.verbose:\n",
    "                print(f\"✗ Error accessing {url}: {str(e)}\")\n",
    "            return url, []\n",
    "\n",
    "    def warm_cache(self):\n",
    "        print(f\"Starting cache warming from {self.start_url}\")\n",
    "        print(f\"Configuration: concurrency={self.concurrency}, delay={self.delay}s, max_pages={self.max_pages}, max_retries={self.max_retries}\")\n",
    "        if self.exclude_paths:\n",
    "            print(f\"Excluding paths containing: {', '.join(self.exclude_paths)}\")\n",
    "        if self.include_paths:\n",
    "            print(f\"Only including paths containing: {', '.join(self.include_paths)}\")\n",
    "        if self.use_sitemap:\n",
    "            print(\"Attempting to load URLs from sitemap.xml...\")\n",
    "            sitemap_urls = self.parse_sitemap(self.start_url, self.exclude_paths, self.include_paths)\n",
    "            if sitemap_urls:\n",
    "                self.queue = []\n",
    "                for url in sitemap_urls[:self.max_pages]:\n",
    "                    self.queue.append(url)\n",
    "                    self.url_from_sitemap[url] = True\n",
    "                print(f\"Using {len(self.queue)} URLs from sitemap.xml\")\n",
    "        self._init_google_sheet()\n",
    "        self.progress_bar = tqdm(total=min(len(self.queue), self.max_pages), desc=\"Warming cache\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=self.concurrency) as executor:\n",
    "            while self.queue and len(self.visited_urls) < self.max_pages:\n",
    "                batch_size = min(self.concurrency, len(self.queue))\n",
    "                batch = [self.queue.pop(0) for _ in range(batch_size)]\n",
    "                batch_set = set(batch)\n",
    "                new_urls = [url for url in batch if url not in self.retry_counts or self.retry_counts[url] == 0]\n",
    "                self.visited_urls.update(new_urls)\n",
    "                future_to_url = {executor.submit(self.visit_url, url): url for url in batch}\n",
    "                for future in concurrent.futures.as_completed(future_to_url):\n",
    "                    url, new_links = future.result()\n",
    "                    retry_links = []\n",
    "                    other_links = []\n",
    "                    for link in new_links:\n",
    "                        if link == url and link in self.retry_counts and self.retry_counts[link] > 0:\n",
    "                            retry_links.append(link)\n",
    "                        elif link not in self.visited_urls and link not in self.queue:\n",
    "                            other_links.append(link)\n",
    "                    if retry_links:\n",
    "                        self.queue = retry_links + self.queue\n",
    "                    if other_links:\n",
    "                        for link in other_links:\n",
    "                            self.queue.append(link)\n",
    "                            if link not in self.url_from_sitemap:\n",
    "                                self.url_from_sitemap[link] = False\n",
    "                    if url not in batch_set or url not in self.retry_counts or self.retry_counts[url] == 0:\n",
    "                        self.progress_bar.update(1)\n",
    "                    if self.delay > 0:\n",
    "                        random_modifier = 2 * random.random() - 1\n",
    "                        actual_delay = max(self.delay * (1 + random_modifier), self.delay * 0.2)\n",
    "                        time.sleep(actual_delay)\n",
    "                if len(self.visited_urls) - self.last_updated_count >= self.update_interval:\n",
    "                    self._update_google_sheet_incremental()\n",
    "                log_system_metrics()\n",
    "        self.progress_bar.close()\n",
    "        print(f\"\\nCache warming completed!\")\n",
    "        print(f\"Visited {len(self.visited_urls)} pages.\")\n",
    "        self._update_google_sheet_incremental()\n",
    "        return self.display_results()\n",
    "\n",
    "    def _save_results_to_csv(self, results_df):\n",
    "        import os\n",
    "        from datetime import datetime\n",
    "        from urllib.parse import urlparse\n",
    "        domain = urlparse(self.start_url).netloc\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        try:\n",
    "            drive_folder_path = os.path.join('/content/drive/Shared drives/Operations/Cache Warmer/', domain)\n",
    "            os.makedirs(drive_folder_path, exist_ok=True)\n",
    "            drive_file_path = os.path.join(drive_folder_path, f\"{timestamp}.csv\")\n",
    "            results_df.to_csv(drive_file_path, index=False)\n",
    "            print(f\"Results saved to Google Drive: {drive_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Google Drive: {str(e)}\")\n",
    "        local_file_path = f\"cache_warming_{domain}_{timestamp}.csv\"\n",
    "        results_df.to_csv(local_file_path, index=False)\n",
    "\n",
    "    def _save_results_to_google_sheet(self, results_df):\n",
    "        from google.colab import auth\n",
    "        import gspread\n",
    "        from google.auth import default\n",
    "        from datetime import datetime\n",
    "        from urllib.parse import urlparse\n",
    "        domain = urlparse(self.start_url).netloc\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        try:\n",
    "            auth.authenticate_user()\n",
    "            creds, _ = default()\n",
    "            gc = gspread.authorize(creds)\n",
    "            sheet_name = f\"{domain} - {timestamp}\"\n",
    "            operations_folder_id = \"1Om8LfyP7WVRGT07KK4ggn5CA7-HfjlaF\"\n",
    "            from googleapiclient.discovery import build\n",
    "            drive_service = build('drive', 'v3', credentials=creds)\n",
    "            file_metadata = {\n",
    "                'name': sheet_name,\n",
    "                'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
    "                'parents': [operations_folder_id]\n",
    "            }\n",
    "            file = drive_service.files().create(\n",
    "                body=file_metadata,\n",
    "                fields='id, webViewLink',\n",
    "                supportsAllDrives=True\n",
    "            ).execute()\n",
    "            spreadsheet = gc.open_by_key(file.get('id'))\n",
    "            worksheet = spreadsheet.get_worksheet(0)\n",
    "            data = [results_df.columns.tolist()]\n",
    "            data.extend(results_df.values.tolist())\n",
    "            worksheet.update(data)\n",
    "            print(f\"Results saved to Google Sheet: {file.get('webViewLink')}\")\n",
    "            return file.get('webViewLink')\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Google Sheet: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def display_results(self):\n",
    "        if not self.page_times:\n",
    "            return pd.DataFrame()\n",
    "        data = {\n",
    "            'URL': list(self.visited_urls),\n",
    "            'Load Time (s)': [self.page_times.get(url) for url in self.visited_urls],\n",
    "            'Status': [str(self.status_codes.get(url)) for url in self.visited_urls],\n",
    "            'Retries': [self.retry_counts.get(url, 0) for url in self.visited_urls],\n",
    "            'URL Source': [(\"Sitemap\" if self.url_from_sitemap.get(url, False) else \"Discovered\") for url in self.visited_urls]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.sort_values('Load Time (s)', ascending=False)\n",
    "        df_plot = df[df['Load Time (s)'].notna()]\n",
    "        if not df_plot.empty:\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            plt.subplot(2, 2, 1)\n",
    "            plt.hist(df_plot['Load Time (s)'], bins=20, color='skyblue', edgecolor='black')\n",
    "            plt.title('Page Load Time Distribution')\n",
    "            plt.xlabel('Load Time (seconds)')\n",
    "            plt.ylabel('Number of Pages')\n",
    "            plt.subplot(2, 2, 2)\n",
    "            status_counts = df['Status'].value_counts()\n",
    "            plt.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n",
    "            plt.title('HTTP Status Codes')\n",
    "            plt.subplot(2, 2, 3)\n",
    "            retry_counts = df['Retries'].value_counts().sort_index()\n",
    "            if not retry_counts.empty:\n",
    "                plt.bar(retry_counts.index.astype(str), retry_counts.values, color='salmon')\n",
    "                plt.title('Retry Distribution')\n",
    "                plt.xlabel('Number of Retries')\n",
    "                plt.ylabel('Number of Pages')\n",
    "            if 'Retries' in df.columns and any(df['Retries'] > 0):\n",
    "                plt.subplot(2, 2, 4)\n",
    "                successful_retries = df[(df['Retries'] > 0) & (df['Load Time (s)'].notna())]\n",
    "                if not successful_retries.empty:\n",
    "                    plt.text(0.5, 0.5,\n",
    "                             f\"Pages requiring retries: {len(df[df['Retries'] > 0])}\\n\" +\n",
    "                             f\"Successfully loaded after retry: {len(successful_retries)}\\n\" +\n",
    "                             f\"Pages that failed all retries: {len(df[(df['Retries'] >= self.max_retries) & (df['Load Time (s)'].isna())])}\",\n",
    "                             horizontalalignment='center',\n",
    "                             verticalalignment='center',\n",
    "                             fontsize=12,\n",
    "                             transform=plt.gca().transAxes)\n",
    "                plt.title('Retry Statistics')\n",
    "                plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        print(\"\\nTop 10 Slowest Pages:\")\n",
    "        display(df.head(10))\n",
    "        if 'Retries' in df.columns and any(df['Retries'] > 0):\n",
    "            print(\"\\nPages that Required Retries:\")\n",
    "            display(df[df['Retries'] > 0])\n",
    "        self._save_results_to_csv(df)\n",
    "        sheet_url = self._save_results_to_google_sheet(df)\n",
    "        return df\n",
    "\n",
    "def run_cache_warmer(url, concurrency=5, delay=0.5, max_pages=100, use_sitemap=True,\n",
    "                     max_retries=3, exclude_paths=None, include_paths=None, find_links=True,update_interval=50,\n",
    "        continue_from_last=False):\n",
    "    \"\"\"Run the cache warmer\n",
    "\n",
    "    Args:\n",
    "        url (str): The website URL to warm cache for\n",
    "        concurrency (int): Number of concurrent requests\n",
    "        delay (float): Delay between requests in seconds\n",
    "        max_pages (int): Maximum number of pages to crawl\n",
    "        use_sitemap (bool): Whether to use sitemap.xml instead of crawling\n",
    "        max_retries (int): Number of times to retry pages that timeout\n",
    "        exclude_paths (list): List of path segments to exclude (e.g. ['library', 'admin'])\n",
    "        include_paths (list): List of path segments to specifically include (e.g. ['about-us'])\n",
    "        find_links (bool): Whether to extract links from each page\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Results of the cache warming process\n",
    "    \"\"\"\n",
    "    warmer = SiteCacheWarmer(\n",
    "        url,\n",
    "        concurrency=concurrency,\n",
    "        delay=delay,\n",
    "        max_pages=max_pages,\n",
    "        verbose=verbose,\n",
    "        use_sitemap=use_sitemap,\n",
    "        max_retries=max_retries,\n",
    "        exclude_paths=exclude_paths,\n",
    "        include_paths=include_paths,\n",
    "        find_links=find_links,\n",
    "        update_interval=update_interval,\n",
    "        continue_from_last=continue_from_last\n",
    "    )\n",
    "\n",
    "    return warmer.warm_cache()\n",
    "\n",
    "exclude_list = [x.strip() for x in exclude_paths.split(\",\")] if exclude_paths else None\n",
    "include_list = [x.strip() for x in include_paths.split(\",\")] if include_paths else None\n",
    "\n",
    "results = run_cache_warmer(\n",
    "    url=url,\n",
    "    concurrency=concurrency,\n",
    "    delay=delay,\n",
    "    max_pages=max_pages,\n",
    "    use_sitemap=use_sitemap,\n",
    "    max_retries=3,\n",
    "    exclude_paths=exclude_list,\n",
    "    include_paths=include_list,\n",
    "    find_links=find_links,\n",
    "    update_interval=update_sheet_interval,\n",
    "    continue_from_last=continue_from_last\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
